#!/usr/bin/env python
# coding: utf-8
"""script that generates source data csvs for searchstims training history figures"""
from argparse import ArgumentParser
from pathlib import Path

import pandas as pd
import pyprojroot

from searchnets.tensorboard import logdir2df


def get_net_number_from_dirname(dirname):
    return dirname.split('_')[-1]


# found a bug in the searchnets function, copying and pasting here with a fix
# so as to not deal with releasing a new version etc. just for this bug
def logdir2csv(logdir):
    """convert tensorboard events files in a logs directory into a .csv file

    Parameters
    ----------
    logdir : str, Path
        path to directory containing tfevents file(s) saved by a SummaryWriter

    Returns
    -------
    None
    """
    logdir = Path(logdir)
    events_files = sorted(logdir.glob('*tfevents*'))
    # remove .csv files -- we can just overwrite them
    events_files = [path for path in events_files if not str(path).endswith('.csv')]
    if len(events_files) != 1:
        if len(events_files) < 1:
            raise ValueError(
                f'did not find any events files in {logdir}'
            )
        elif len(events_files) > 1:
            raise ValueError(
                f'found multiple events files in {logdir}:\n{events_files}.'
                'Please ensure there is only one events file in the directory, '
                'unclear which to use.'
            )
    else:
        events_file = events_files[0]

    df = logdir2df(logdir)

    csv_filename = events_file.stem + '.csv'
    df.to_csv(logdir.joinpath(csv_filename))


def main(ckpt_root,
         source_data_root,
         csv_filename,
         net_names,
         methods,
         modes,
         loss_funcs,
         ):
    """generate .csv files used as source data for figures corresponding to experiments
    carried out with stimuli generated by searchstims library

    Parameters
    ----------
    ckpt_root : str, Path
        path to root of directory that has checkpoints saved for a specific experiment.
        This script expects the following structure:
        ./results/VSD/checkpoints/{ckpt_root}/{net_name}{experiment_suffix}
    source_data_root : str, Path
        path to root of directory where csv files
        that are the source data for figures should be saved.
    csv_filename : str
        filename for .csv saved that contains results from **all** results.gz files.
        Saved in source_data_root.
    net_names : list
        of str, neural network architecture names
    methods : list
        of str,  training "methods". Valid values are {"transfer", "initialize"}.
    modes : list
        of str, training "modes". Valid values are {"classify","detect"}.
    loss_funcs : list
        of str, loss functions. Valid values are {"BCE", "CE-largest", "CE-random"}.
    """
    ckpt_root = Path(ckpt_root)
    source_data_root = Path(source_data_root)
    # directories in checkpoint root will be named (basically) {net}_{method}_{learning_rate}_{dataset}
    # 'mode' is implied by whether or not 'detect' appears in directory name
    # we need to get training history for each {net}_{method} & mode
    net_ckpt_roots = [path for path in sorted(ckpt_root.iterdir()) if path.is_dir()]

    # loop that saves a separate .csv for each net / method / mode / loss func
    for net_name in net_names:
        this_net_ckpt_roots = [path for path in net_ckpt_roots if net_name in str(path)]

        for method in methods:  # transfer learning? or train from randomly-initialized weights?
            if method not in METHODS:
                raise ValueError(
                    f'invalid method: {method}, must be one of: {METHODS}'
                )

            this_method_ckpt_roots = [ckpt_root
                                      for ckpt_root in this_net_ckpt_roots
                                      if method in ckpt_root.name]

            for mode in modes:
                if mode == 'classify':
                    this_mode_ckpt_roots = [ckpt_root
                                            for ckpt_root in this_method_ckpt_roots
                                            if 'detect' not in ckpt_root.name]
                elif mode == 'detect':
                    this_mode_ckpt_roots = [ckpt_root
                                            for ckpt_root in this_method_ckpt_roots
                                            if 'detect' in ckpt_root.name]
                else:
                    raise ValueError(
                        f'invalid mode: {mode}, must be one of: {MODES}'
                    )

                for loss_func in loss_funcs:
                    this_loss_func_ckpt_roots = [ckpt_root
                                                 for ckpt_root in this_mode_ckpt_roots
                                                 if loss_func in ckpt_root.name
                                                 ]
                    if len(this_loss_func_ckpt_roots) == 0:
                        print(
                            "did not find only any checkpoint roots "
                            f"for net '{net_name}', method '{method}', mode '{mode}', and loss func '{loss_func}'."
                            f"Will continue without generating training history .csv files."
                        )
                        continue

                    if len(this_loss_func_ckpt_roots) > 1:
                        raise ValueError(
                            "did not find only a single checkpoint root "
                            f"for net '{net_name}', method '{method}', mode '{mode}', and loss func '{loss_func}'."
                            f"Instead found:\n{this_loss_func_ckpt_roots}"
                        )
                    this_ckpt_root = this_loss_func_ckpt_roots[0]

                    # the '**' in the glob in the next line will usually be 'trained_200_epochs'
                    # even though early stopping was used and nets weren't always trained 200 epochs
                    net_roots = sorted(this_ckpt_root.glob('**/net_number*'))
                    df_all_net_numbers = []
                    for net_root in net_roots:  # here 'net' refers to the training replicate
                        print(
                            f'processing training history for:\n\t{net_root}'
                        )
                        net_number = int(
                            get_net_number_from_dirname(net_root.name)
                        )

                        # convert to .csv -- do this every time even though it takes longer
                        # to make sure we have converted **all** events files, and that we
                        # 're-generate' the .csv files from the 'ground truth' events files
                        events_file = sorted(net_root.glob('**/*events*'))
                        events_file = [path for path in events_file if not str(path).endswith('.csv')]
                        assert len(events_file) == 1, 'found more than one events file'
                        events_file = events_file[0]
                        logdir = events_file.parent
                        logdir2csv(logdir)

                        # now load the .csv file we just created
                        events_csv = sorted(net_root.glob('**/*events*csv'))
                        assert len(events_csv) == 1, 'found more than one events file'
                        events_csv = events_csv[0]
                        df = pd.read_csv(events_csv)
                        df['replicate'] = net_number
                        df['net_name'] = net_name
                        df['method'] = method
                        df['mode'] = mode
                        df['loss_func'] = loss_func
                        df_all_net_numbers.append(df)
                    df_all_net_numbers = pd.concat(df_all_net_numbers)

                    # save a separate .csv for each net / method / mode / loss func
                    stem, ext = Path(csv_filename).stem, Path(csv_filename).suffix
                    this_csv_filename = f'{stem}-{net_name}-{mode}-{method}-{loss_func}{ext}'
                    df_all_net_numbers.to_csv(
                        source_data_root.joinpath(this_csv_filename), index=False
                    )


ROOT = pyprojroot.here()
DATA_DIR = ROOT.joinpath('data')
RESULTS_ROOT = ROOT.joinpath('results')

VSD_ROOT = RESULTS_ROOT.joinpath('VSD')
CHECKPOINT_ROOT = VSD_ROOT.joinpath('checkpoints')
SOURCE_DATA_ROOT = VSD_ROOT.joinpath('source_data')

NET_NAMES = [
    'alexnet',
    'VGG16',
    'CORnet_Z',
    'CORnet_S',
]

METHODS = [
    'initialize',
    'transfer'
]

MODES = ['classify']

LOSS_FUNCS = [
    "BCE",
    "CE_largest",
    "CE_random"
]


def get_parser():
    parser = ArgumentParser()
    parser.add_argument('--ckpt_root', default=CHECKPOINT_ROOT,
                        help='path to root of directory that has checkpoint files created by searchstims train command')
    parser.add_argument('--source_data_root', default=SOURCE_DATA_ROOT,
                        help=('path to root of directory where "source data" csv files '
                              'that are generated should be saved'))
    parser.add_argument('--all_csv_filename', default='training_history.csv',
                        help=('filename for .csv that should be saved '
                              'that contains results from **all** results.gz files. '
                              'Saved in source_data_root.'))
    parser.add_argument('--net_names', default=NET_NAMES,
                        help='comma-separated list of neural network architecture names',
                        type=lambda net_names: net_names.split(','))
    parser.add_argument('--methods', default=METHODS,
                        help='comma-separated list of training "methods", must be in {"transfer", "initialize"}',
                        type=lambda methods: methods.split(','))
    parser.add_argument('--modes', default=MODES,
                        help='comma-separated list of training "modes", must be in {"classify","detect"}',
                        type=lambda modes: modes.split(','))
    parser.add_argument('--loss_funcs', default=LOSS_FUNCS,
                        help=('comma-separated list of loss function names, '
                              'must be in {"BCE", "CE-largest", "CE-random"}'),
                        type=lambda loss_funcs: loss_funcs.split(','))

    return parser


if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()
    main(ckpt_root=args.ckpt_root,
         source_data_root=args.source_data_root,
         csv_filename=args.all_csv_filename,
         net_names=args.net_names,
         methods=args.methods,
         modes=args.modes,
         loss_funcs=args.loss_funcs,
         )
