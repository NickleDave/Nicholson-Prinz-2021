# dummy config just used to split dataset after generating it
[DATA]
TRAIN_SIZE = 102400
STIM_TYPES = ['RVvGV', 'RVvRHGV', '2_v_5', 'YT_v_BTYL', 'YT_v_BTBL', 'Bx_v_RxBo', 'Bx_v_RxRo', 'TvT']
VALIDATION_SIZE = 1600
TEST_SIZE = 6400
SET_SIZES = [1, 2, 4, 8]
CSV_FILE_IN = ../visual_search_stimuli/alexnet_multiple_stims_v2/alexnet_multiple_stims.csv
CSV_FILE_OUT = ../visual_search_stimuli/alexnet_multiple_stims_v2/alexnet_multiple_stims_balanced_split.csv
TRAIN_SIZE_PER_SET_SIZE = [3200, 3200, 3200, 3200]

[TRAIN]
NETNAME = alexnet
EPOCHS = 200
RANDOM_SEED = 42
BATCH_SIZE = 64
SAVE_PATH = results/searchstims/checkpoints/10stims/alexnet_transfer_lr_1e-03_no_finetune_multiple_stims_128000samples_balanced
METHOD = transfer
BASE_LEARNING_RATE = 1e-20
FREEZE_TRAINED_WEIGHTS = True
NEW_LAYER_LEARNING_RATE = 0.001
NEW_LEARN_RATE_LAYERS = ['fc8']
NUMBER_NETS_TO_TRAIN = 8
SAVE_ACC_BY_SET_SIZE_BY_EPOCH = False
USE_VAL = True
VAL_STEP = 50
PATIENCE = 20
CKPT_STEP = 200
NUM_WORKERS = 32
DATA_PARALLEL = True
SUMMARY_STEP = 1

[TEST]
TEST_RESULTS_SAVE_PATH = results/searchstims/results_gz/10stims/alexnet_transfer_lr_1e-03_no_finetune_multiple_stims_128000samples_balanced

