"""PyTorch Dataset class for VOC Detection task, adapted to use with Visual Search Difficulty dataset

adapted from Torchvision
https://github.com/pytorch/vision
under BSD-3 license
https://github.com/pytorch/vision/blob/master/LICENSE

VocAnnotationTransform adapted from
https://github.com/amdegroot/ssd.pytorch/blob/master/data/voc0712.py
under MIT license
https://github.com/amdegroot/ssd.pytorch/blob/master/LICENSE
"""
from pathlib import Path

import numpy as np
import pandas as pd
from PIL import Image
from torchvision.datasets import VisionDataset


class SearchstimsDetection(VisionDataset):
    """`Searchstims`_ Detection Dataset."""
    def __init__(self,
                 root,
                 csv_file,
                 split='train',
                 transform=None,
                 target_transform=None,
                 transforms=None):
        """
        Parameters
        ----------
        root : str
            Root directory of the VOC Dataset.
        csv_file : str
            name of .csv file generated by searchnets.data.split
        split : str
            Split of entire dataset to use. One of {'train', 'val', 'test'}.
        image_set : str
            Select the image_set to use, ``train``, ``trainval`` or ``val``.
            Default is ``train``.
        transform : callable
            A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform : callable
            A function/transform that takes in the
            target and transforms it.
        transforms : callable
            A function/transform that takes input sample and its target as entry
            and returns a transformed version.
        """
        super(SearchstimsDetection, self).__init__(root, transforms, transform, target_transform)

        if split not in {'train', 'val', 'test'}:
            raise ValueError("split must be one of: {'train', 'val', 'test'}")

        self.csv_file = csv_file
        self.transform = transform
        self.split = split
        df = pd.read_csv(csv_file)
        df = df[df['split'] == split]
        self.df = df

        img_files = df['img_file'].values
        root_output_dir = df['root_output_dir'].values
        self.images = [
            str(Path(root).joinpath(img_file))
            for root, img_file in zip(root_output_dir, img_files)
        ]

        xml_files = df['xml_file'].values
        self.annotations = [
            str(Path(root).joinpath(xml_file))
            for root, xml_file in zip(root_output_dir, xml_files)
        ]
        assert (len(self.images) == len(self.annotations))

        target_condition = df['target_condition'].values
        target_condition = np.asarray(
            [1 if tc == 'present' else 0 for tc in target_condition]
        )
        self.target_condition = target_condition

        self.transform = transform
        self.target_transform = target_transform

        self.set_size = df['set_size'].values

    def __getitem__(self, index):
        """
        Parameters
        ----------
        index : int
            index of training sample in dataset

        Returns
        -------
        item : dict
            with following key, value pairs:
                image : torch.Tensor
                    image as a tensor
                classes : torch.Tensor
                    one-hot encoding of what objects are present in the image.
                largest : torch.Tensor
                    of integers, largest class present in the image as determined by the area of its bounding box
                random : torch.Tensor
                    of integers, a random class present in the image
                index : torch.Tensor
                    indices used to get images and targets, can be used to obtain filenames
                    from VOCTestDataset.images attribute
                vsd_score : torch.Tensor
                    float values, Visual Search Difficulty score for each image,
                    from the Visual Search Difficulty dataset.
        """
        img_path = self.images[index]
        img = Image.open(img_path).convert('RGB')
        target = self.annotations[index]

        item = {
            'img': self.transform(img),
            'name': Path(img_path).stem,
            'target': self.target_transform(target),
            'index': index,
        }

        return item

    def __len__(self):
        return len(self.images)
